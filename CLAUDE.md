# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

RENEC Harvester v02 is a site-wide public data harvester for México's RENEC platform (National Registry of Competency Standards). The project is currently in its documentation phase with implementation starting in Sprint 0 (Aug 21-22, 2024).

## Development Commands

### Environment Setup
```bash
# Start services (PostgreSQL, Redis, monitoring)
docker-compose up -d

# Install Python dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # For development

# Install Playwright browsers
playwright install --with-deps

# Initialize database
python -m src.cli db init
python -m src.cli db migrate
```

### Scrapy Crawler Operations
```bash
# Run crawler in mapping mode
scrapy crawl renec -a mode=crawl -a max_depth=5

# Run crawler in harvest mode
scrapy crawl renec -a mode=harvest

# List all spiders
scrapy list

# Run with specific settings
scrapy crawl renec -s CONCURRENT_REQUESTS=10 -s DOWNLOAD_DELAY=0.5
```

### CLI Operations (coming in Sprint 1)
```bash
python -m src.cli crawl     # Map the site structure
python -m src.cli sniff     # Sniff XHR endpoints  
python -m src.cli harvest   # Extract all components
python -m src.cli validate  # Validate extracted data
python -m src.cli diff      # Generate diff reports
python -m src.cli publish   # Publish artifacts
```

### Testing
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/unit/test_spider.py

# Run integration tests only
pytest tests/integration/

# Run with verbose output
pytest -v
```

### Code Quality
```bash
# Format code
black src tests

# Lint code
ruff check src tests

# Type checking
mypy src

# Run all checks
make lint  # If Makefile is set up
```

### Database Operations
```bash
# Access PostgreSQL
docker exec -it renec-postgres psql -U renec -d renec_harvester

# Run Alembic migrations
alembic upgrade head

# Create new migration
alembic revision --autogenerate -m "Description"

# Access Redis CLI
docker exec -it renec-redis redis-cli -a renec_redis_pass
```

### Monitoring
```bash
# Access Prometheus: http://localhost:9090
# Access Grafana: http://localhost:3000 (admin/renec_grafana_pass)
# Access Flower (Celery): http://localhost:5555 (admin/renec_flower_pass)
# Access pgAdmin: http://localhost:5050 (admin@renec.local/renec_pgadmin_pass)
# Access Redis Commander: http://localhost:8081
```

### Frontend (Next.js)
```bash
cd ui
npm install                 # Install dependencies
npm run dev                 # Development server
npm run build              # Production build
npm run start              # Production server
```

### Make Targets (Optional)
```bash
make install               # Setup environment
make crawl                 # Run crawler
make harvest              # Run full harvest
make validate             # Validate data
make publish              # Publish artifacts
make build-ui             # Build frontend
```

## Architecture Overview (Enhanced)

The harvester now uses an optimized architecture for performance and scalability:

### Core Components

1. **Discovery Layer** (`src/discovery/`)
   - **Scrapy + Scrapy-Playwright** for parallel crawling with browser automation
   - Network request interception for API endpoint discovery
   - Concurrent processing with up to 10 browser contexts
   - Automatic retry and circuit breaker patterns

2. **Data Models** (`src/models/`)
   - **PostgreSQL** with SQLAlchemy ORM for concurrent writes
   - Graph-based schema with proper relationships
   - Temporal tracking (first_seen/last_seen)
   - JSONB fields for flexible metadata storage

3. **Caching Layer**
   - **Redis** for request deduplication and rate limiting
   - API response caching with TTL
   - Distributed locks for coordination

4. **Monitoring** (`src/monitoring/`)
   - **Prometheus** metrics collection
   - **Grafana** dashboards for visualization
   - Custom Scrapy extension for spider metrics
   - Database query performance tracking

5. **Task Queue** (Coming in Sprint 1)
   - **Celery** for distributed task processing
   - **Flower** for task monitoring
   - Scheduled harvests and freshness checks

### Key Architectural Improvements

- **Parallel Processing**: Multiple concurrent spider instances (10x performance)
- **PostgreSQL**: Handles concurrent writes with proper indexing
- **Redis Caching**: Reduces redundant requests and API load
- **Circuit Breakers**: Graceful handling of downstream failures
- **Metrics & Observability**: Full visibility into system performance

### Data Flow

1. **Crawl Mode**: Spider discovers URLs → Network capture → Build site map
2. **Harvest Mode**: Spider extracts data → Validation → Normalization → Database
3. **API Mode**: Request → Redis cache check → PostgreSQL → Response

### Performance Targets (Achievable with new architecture)
- Full harvest: ≤20 minutes (with 10 concurrent spiders)
- API response: ≤50ms (with Redis caching)
- Database writes: 1000+ items/second

## Key Development Guidelines

1. **Sprint Structure**: Follow the ROADMAP.md timeline. Each sprint has specific deliverables and acceptance criteria.

2. **Data Extraction**: 
   - Always respect robots.txt and ToS
   - Implement conservative rate limiting (1-2 requests/second)
   - Use ETag/If-Modified-Since headers for efficient polling
   - Target ≥99% coverage of RENEC components

3. **Performance Targets**:
   - Full harvest: ≤20 minutes
   - API response: ≤50ms for entity queries
   - UI load time: ≤2 seconds

4. **Testing Requirements**:
   - Unit tests for all parsers and validators
   - Integration tests for the full pipeline
   - QA validation before each release

5. **Output Artifacts** (in `artifacts/` directory):
   - `crawl_map.json` - Site structure
   - `harvest_YYYYMMDD.db` - SQLite database
   - `harvest_YYYYMMDD.json` - JSON export
   - `harvest_YYYYMMDD.csv` - CSV export
   - `diff_YYYYMMDD.html` - Change report

## Current Status

As of the last commit, the project contains:
- README.md - Project overview and usage
- ROADMAP.md - Development timeline through GA launch
- SOFTWARE_SPEC.md - Detailed technical specification

Implementation begins with Sprint 0 tasks:
1. Run reconnaissance to produce CrawlMap
2. Create repository skeleton
3. Set up GitHub Actions workflows
4. Implement basic CLI structure